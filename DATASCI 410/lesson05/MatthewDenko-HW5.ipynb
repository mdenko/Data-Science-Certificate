{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 \n",
    "\n",
    "## Author - Matthew Denko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "    Using the automotive data set, create a new notebook or use Studentname-M02-HypothesisSim.ipynb to perform the  following:\n",
    "\n",
    "    1. Identify a likely distribution for price and several other features.\n",
    "    2. Compute basic summary statistics by both classical, bootstrap and Bayesian methods\n",
    "    3. Compute confidence intervals for the above summary statistics by classical, bootstrap and Bayesian methods\n",
    "    4. Summarize your findings and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Identify a likely distribution for price and several other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing/Cleaning Data/Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions\n",
    "\n",
    "def plot_hist(x, p=5):\n",
    "    # Plot the distribution and mark the mean\n",
    "    pyplot.hist(x, alpha=.5)\n",
    "    pyplot.axvline(x.mean())\n",
    "    # 95% confidence interval    \n",
    "    pyplot.axvline(np.percentile(x, p/2.), color='red', linewidth=3)\n",
    "    pyplot.axvline(np.percentile(x, 100-p/2.), color='red', linewidth=3)\n",
    "    \n",
    "def bern_pmf(x, p):\n",
    "    if (x == 1):\n",
    "        return p\n",
    "    elif (x == 0):\n",
    "        return 1 - p\n",
    "    else:\n",
    "        return \"Value Not in Support of Distribution\"\n",
    "    \n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), st.sem(a)\n",
    "    h = se * st.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return print(\"The mean is:\",m, \"The lower bound is:\",m-h, \"The upper bound is\",m+h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "\n",
    "url = \"https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/Automobile%20price%20data%20_Raw_.csv\"\n",
    "Auto = pd.read_csv(url, header=None)\n",
    "\n",
    "#Assigning Column Names\n",
    "\n",
    "Auto.columns = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\", \"body-style\", \"drive-wheels\",\n",
    "               \"engine-location\", \"wheel-base\",\"length\", \"width\", \"height\", \"curb-weight\", \"engine-type\", \"num-of=cylinders\",\n",
    "               \"engine-size\", \"fuel-system\", \"bore\", \"stroke\", \"compression-ratio\",\"horsepower\",\"peak-rpm\",\"city-mpg\",\n",
    "               \"highway-mpg\",\"price\"]\n",
    "print(Auto.columns)\n",
    "print(Auto.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "\n",
    "print(Auto.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cases with missing data\n",
    "\n",
    "Auto.loc ['price',:] = pd.to_numeric(Auto['price'], errors='coerce').fillna(0)\n",
    "Auto = Auto.replace(to_replace= \"?\", value= float('NaN'))\n",
    "\n",
    "# Dropping rows with nulls\n",
    "\n",
    "Auto = Auto.dropna(axis = 0)\n",
    "Auto_null = Auto.isnull().sum()\n",
    "print(\"\"\"Null Counts by Column\n",
    "\"\"\",Auto_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Data to numeric\n",
    "\n",
    "#price\n",
    "Auto.loc [:,'price'] = pd.to_numeric(Auto['price'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'price'] = Auto['price'].astype('float')\n",
    "\n",
    "#horsepower\n",
    "Auto.loc [:,'horsepower'] = pd.to_numeric(Auto['horsepower'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'horsepower'] = Auto['horsepower'].astype('float')\n",
    "\n",
    "#height\n",
    "Auto.loc [:,'height'] = pd.to_numeric(Auto['height'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'height'] = Auto['height'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    All null values have now been removed from the dataset. And the feautures that I want to explore further have been converted numeric values. I will be focusing my analysis on three features: price, horsepower, and height. In the next section I will be taking a graphical look at their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    For the identifying of a likely distribution for the three features I chose (price, horsepower, and height) I want to convert them each to probability values and create a new subsetted dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data Frame for necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Data to numeric\n",
    "\n",
    "#price\n",
    "Auto.loc [:,'price'] = pd.to_numeric(Auto['price'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'price'] = Auto['price'].astype('float')\n",
    "\n",
    "#horsepower\n",
    "Auto.loc [:,'horsepower'] = pd.to_numeric(Auto['horsepower'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'horsepower'] = Auto['horsepower'].astype('float')\n",
    "\n",
    "#height\n",
    "Auto.loc [:,'height'] = pd.to_numeric(Auto['height'], errors='coerce').fillna(0)\n",
    "Auto.loc[:,'height'] = Auto['height'].astype('float')\n",
    "\n",
    "#Determing Mean Values\n",
    "\n",
    "#price\n",
    "price_mean = Auto['price'].mean()\n",
    "print('Price Mean is: $', price_mean)\n",
    "\n",
    "#horsepower\n",
    "hp_mean = Auto['horsepower'].mean()\n",
    "print('Horespower Mean is :', hp_mean)\n",
    "\n",
    "#height\n",
    "height_mean = Auto['height'].mean()\n",
    "print('Height Mean is:', height_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    I will now create a new dataframe for 1 or 0 indicators of whether or not each of the columns are greater than the interger value of the mean. For price this will be 11,374. For Horsepower this will be 171. For Height this will be 54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating New Data Frame for probabilities\n",
    "\n",
    "auto_new = pd.DataFrame()\n",
    "\n",
    "#price\n",
    "auto_new.loc[:,'price'] = (Auto.loc[:,'price'] > 11374).astype(int)\n",
    "\n",
    "#horsepower\n",
    "auto_new.loc[:,'horsepower'] = (Auto.loc[:,'horsepower'] > 171).astype(int)\n",
    "\n",
    "#height\n",
    "auto_new.loc[:,'height'] = (Auto.loc[:,'height'] > 54).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    Each of the variables have now been converted to probability variables. So for price, horsepower, and height I choose a Bernouili distribution for parameter p, the probability that they are above their mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics\n",
    "\n",
    "print(auto_new.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The likelihood is the probability seeing our data x given the parameter θ this is written as p(X|θ). The likelihood distribution allows is to specify how we think the data was generated. In this case we know how the data was generated and we can think of it as being generated from a Bernoulli Distribution.\n",
    "    \n",
    "    In this distrbution there is a parameter p which is the proability of getting a 1 and the probability of getting a 0 is 1-p. For price that is the probability the price of a car is >11,374 which based off the mean of the new dataset that probability is .34 and probability of not having car with a price above the mean is .66. For horsepower that is the probability of the horsepower of a car > 171 which based off the mean of the new dataset that probility is .01 and the probability of not having a car with horsepower >171 is .99. For height that is the probability that the height of a car >54 which based off the mean of the new dataset that probability is .53 and probability of not having a car with height >54 is .47."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Likehood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price\n",
    "\n",
    "#Computing Likelihood\n",
    "price = auto_new.loc[:,'price']\n",
    "likelihood = np.product(st.bernoulli.pmf(price,.3375))\n",
    "print(\"This is the likelihood of price:\", likelihood)\n",
    "\n",
    "#Graphing Likelihood\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "p_x = [np.product(st.bernoulli.pmf(price, p)) for p in params]\n",
    "plt.plot(params, p_x)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    This graph shows the distribution of the likelihood for price with a sample of 100. As you can see the peak is around .34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horespower\n",
    "\n",
    "#Computing Likelihood\n",
    "hp = auto_new.loc[:,'horsepower']\n",
    "likelihood = np.product(st.bernoulli.pmf(price,.3375))\n",
    "print(\"This is the likelihood of horsepower:\", likelihood)\n",
    "\n",
    "#Graphing Likelihood\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "p_x = [np.product(st.bernoulli.pmf(hp, p)) for p in params]\n",
    "plt.plot(params, p_x)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "        This graph shows the distribution of the likelihood for horsepower with a sample of 100. As you can see the peak is around 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height\n",
    "\n",
    "#Computing Likelihood\n",
    "height = auto_new.loc[:,'height']\n",
    "likelihood = np.product(st.bernoulli.pmf(price,.3375))\n",
    "print(\"This is the likelihood of height:\", likelihood)\n",
    "\n",
    "#Graphing Likelihood\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "p_x = [np.product(st.bernoulli.pmf(height, p)) for p in params]\n",
    "plt.plot(params, p_x)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    This graph shows the distribution of the likelihood for price with a sample of 100. As you can see the peak is around .53."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price\n",
    "\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(price, p)) for p in params])\n",
    "p_fair = p_fair / np.sum(p_fair)\n",
    "plt.plot(params, p_fair)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "      In this case the prior distribution for price is exremely similar to the likelihood distributio for price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horsepower\n",
    "\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(hp, p)) for p in params])\n",
    "p_fair = p_fair / np.sum(p_fair)\n",
    "plt.plot(params, p_fair)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "      In this case the prior distribution for horsepower is exremely similar to the likelihood distributio for horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height\n",
    "\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(height, p)) for p in params])\n",
    "p_fair = p_fair / np.sum(p_fair)\n",
    "plt.plot(params, p_fair)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    In this case the prior distribution for height is exremely similar to the likelihood distributio for height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price\n",
    "\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "likelihood = [np.product(st.bernoulli.pmf(price, p)) for p in params]\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(price, p)) for p in params])\n",
    "prior = p_fair / np.sum(p_fair)\n",
    "posterior = [prior[i] * likelihood[i] for i in range(prior.shape[0])]\n",
    "posterior = posterior / np.sum(posterior)\n",
    "fig, axes = plt.subplots(3, 1, sharex=True, figsize=(8,8))\n",
    "axes[0].plot(params, likelihood)\n",
    "axes[0].set_title(\"Price Sampling Distribution\")\n",
    "axes[1].plot(params, prior)\n",
    "axes[1].set_title(\"Price Prior Distribution\")\n",
    "axes[2].plot(params, posterior)\n",
    "axes[2].set_title(\"Price Posterior Distribution\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horsepower\n",
    "\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "likelihood = [np.product(st.bernoulli.pmf(hp, p)) for p in params]\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(hp, p)) for p in params])\n",
    "prior = p_fair / np.sum(p_fair)\n",
    "posterior = [prior[i] * likelihood[i] for i in range(prior.shape[0])]\n",
    "posterior = posterior / np.sum(posterior)\n",
    "fig, axes = plt.subplots(3, 1, sharex=True, figsize=(8,8))\n",
    "axes[0].plot(params, likelihood)\n",
    "axes[0].set_title(\"Horsepower Sampling Distribution\")\n",
    "axes[1].plot(params, prior)\n",
    "axes[1].set_title(\"Horsepower Prior Distribution\")\n",
    "axes[2].plot(params, posterior)\n",
    "axes[2].set_title(\"Horsepower Posterior Distribution\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Height\n",
    "\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "params = np.linspace(0, 1, 100)\n",
    "likelihood = [np.product(st.bernoulli.pmf(height, p)) for p in params]\n",
    "p_fair = np.array([np.product(st.bernoulli.pmf(height, p)) for p in params])\n",
    "prior = p_fair / np.sum(p_fair)\n",
    "posterior = [prior[i] * likelihood[i] for i in range(prior.shape[0])]\n",
    "posterior = posterior / np.sum(posterior)\n",
    "fig, axes = plt.subplots(3, 1, sharex=True, figsize=(8,8))\n",
    "axes[0].plot(params, likelihood)\n",
    "axes[0].set_title(\"Height Sampling Distribution\")\n",
    "axes[1].plot(params, prior)\n",
    "axes[1].set_title(\"Height Prior Distribution\")\n",
    "axes[2].plot(params, posterior)\n",
    "axes[2].set_title(\"Height Posterior Distribution\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    For each of the features: Price, Height, and Horsepower, they all had very similar likelihood, prior, and posterior distributions. In this case the Baynesian model does not give us many advantages as our own specification of the sampling distribution turned out to be very close to actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap vs Classical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Bootstrap Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap\n",
    "\n",
    "Auto_bootstrap = Auto.sample(frac=1, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "     Bootstrapping is continued resampling with equivalent size and replacement from an original dataset. It allows us to make the sample size larger without generating more results. In the next section I will be comparing a bootstrap dataset with the original sample.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "\n",
    "print('Dataset Summary Statistics:',Auto.describe())\n",
    "\n",
    "#Bootstrap Sample\n",
    "\n",
    "print('Bootstrap Summary Statistics:',Auto_bootstrap.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "\n",
    "# Price - dataset\n",
    "dataset_hist = plot_hist(Auto.loc[:,'price'])\n",
    "print('Dataset Price Histogram')\n",
    "plt.show(dataset_hist)\n",
    "\n",
    "# Price - bootstrap\n",
    "bootstrap_hist = plot_hist(Auto_bootstrap.loc[:,'price'])\n",
    "print('Boostrap Price Histogram')\n",
    "plt.show(bootstrap_hist)\n",
    "\n",
    "# Horsepower - dataset\n",
    "dataset_hist = plot_hist(Auto.loc[:,'horsepower'])\n",
    "print('Dataset Price Histogram')\n",
    "plt.show(dataset_hist)\n",
    "\n",
    "# Horsepower - bootstrap\n",
    "bootstrap_hist = plot_hist(Auto_bootstrap.loc[:,'horsepower'])\n",
    "print('Boostrap Price Histogram')\n",
    "plt.show(bootstrap_hist)\n",
    "\n",
    "# Height - dataset\n",
    "dataset_hist = plot_hist(Auto.loc[:,'height'])\n",
    "print('Dataset Price Histogram')\n",
    "plt.show(dataset_hist)\n",
    "\n",
    "# Height - bootstrap\n",
    "bootstrap_hist = plot_hist(Auto_bootstrap.loc[:,'height'])\n",
    "print('Boostrap Price Histogram')\n",
    "plt.show(bootstrap_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments: \n",
    "\n",
    "    The bootstrap sample appears to be very similar to the dataset however they are still slightly different. The distribution across all three variables is very similar as are the mean values. However, for price the Q3 value is significantly different in the bootstrap sample then in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments: \n",
    "\n",
    "    I will create confidence intervals for both the bootstrap sample and the dataset using a 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price - Dataset\n",
    "\n",
    "price_ci = mean_confidence_interval(Auto.loc[:,\"price\"], confidence=0.95)\n",
    "print(price_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price - Bootstrap\n",
    "\n",
    "price_ci_bs = mean_confidence_interval(Auto_bootstrap.loc[:,\"price\"], confidence=0.95)\n",
    "print(price_ci_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horsepower - Dataset\n",
    "\n",
    "hp_ci = mean_confidence_interval(Auto.loc[:,\"horsepower\"], confidence=0.95)\n",
    "print(hp_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horsepower - Bootstrap\n",
    "\n",
    "hp_ci_bs = mean_confidence_interval(Auto_bootstrap.loc[:,\"horsepower\"], confidence=0.95)\n",
    "print(hp_ci_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Height - Dataset\n",
    "\n",
    "height_ci = mean_confidence_interval(Auto.loc[:,\"height\"], confidence=0.95)\n",
    "print(height_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Height - Bootstrap\n",
    "\n",
    "height_ci_bs = mean_confidence_interval(Auto_bootstrap.loc[:,\"height\"], confidence=0.95)\n",
    "print(height_ci_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    A confidence interval can be interpretted as if we repeated this process many many times then about 95% of the invervals captured will capture the true mean. In this case, most of the variables have very simimlar confidence intervals except for price. The dataset and bootstrap sample have largely different bounds. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
