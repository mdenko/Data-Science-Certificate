{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4 - Independent Project\n",
    "\n",
    "## Author - Matthew Denko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "    Milestone 4 is where everything you built in the previous milestones comes together. For this Milestone, you focus on improving model accuracy and summarizing your findings. Try explaining your findings as if you are presenting to your management team in laymanâ€™s terms. For example, talk about the influencing factors, what can be improved, what is important in your findings, what is the key aspect to focus on, what do the data tell them that they do not know.\n",
    "\n",
    "    For Milestone 4 you should:\n",
    "\n",
    "    (1) update Milestones 1 through 3, and assignment 9 based on feedback;\n",
    "    (2) enhance your model results by trying different model and/or data enhancement techniques (Build 3 models with different enhancements and feature engineering techniques);\n",
    "    (3) explain your choice of model and model accuracy; and\n",
    "    (4) draw direct inferences and conclusions from model results (Describe how your model results can improve or provide a solution to the problem you have chosen).\n",
    "    (5) Use graph and evidence from the data to prove your point. Part of being a data scientist is to tell a story that helps the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This dataset contains demographic data from the 1994 Census database which was gathered to see if it could predict if an Adult makes >50k annually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Running Model:\n",
    "\n",
    "    I want to examine how demographic variables influence wealth metrics such as captial gains or income bracket. The goal of this analysis is to see if we can accurately predict someone's captial gain or whether or not they make greater than 50k annually. The results of this analysis could be used to identify members of society that might be more susceptible to making lower income or low capital gains. They could also be used to identify areas which are more likely to have high wealth metrics such as education and encourage people to explore these areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How I Will Attempt To Improve My Results:\n",
    "\n",
    "    I will attempt to improve the accuracy score and R2 value of my model by trying 3 different techniques:\n",
    "    \n",
    "    (1) I will add more features to my multiple linear regression model to see if they have better explanatory power. I will then use backwards selection to remove the features that do not add value. \n",
    "    \n",
    "    (2) I will try a PCR model using the same list of features.\n",
    "    \n",
    "    (3) I will see if these variables are a better predictor of a different variable >50K, <=50k (whether or not someone makes more than 50 k anually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Citation\n",
    "\n",
    "source_citation = \"Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\"\n",
    "print(\"source citation = \",source_citation)\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets/Adult'\n",
    "print(\"url =\",url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sms\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions\n",
    "\n",
    "# Scale function\n",
    "\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading url\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "Adult= pd.read_csv(url, header=None)\n",
    "\n",
    "# Assigning reasonable column names\n",
    "\n",
    "Adult.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"educationnum\",\"maritalstatus\",\"occupation\",\n",
    "                 \"relationship\",\"race\",\"sex\",\"capitalgain\",\"capitalloss\",\"hoursperweek\",\"nativecountry\",\">50K, <=50k\"]\n",
    "print(Adult.columns)\n",
    "Adult.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Unecessary Columns\n",
    "\n",
    "Adult = Adult.drop(columns=['fnlwgt', 'workclass', 'relationship','capitalloss','>50K, <=50k', 'education','race'])\n",
    "print(Adult.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cases with missing data\n",
    "\n",
    "Adult = Adult.replace(to_replace= \"?\", value=float(\"NaN\"))\n",
    "Adult_null = Adult.isnull().sum()\n",
    "print(Adult_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Hot Encode Categorical Data\n",
    "\n",
    "onehot_features = ['maritalstatus','occupation', 'sex','nativecountry']\n",
    "dummy = pd.get_dummies(Adult[onehot_features])\n",
    "dummy.head()\n",
    "Adult = pd.concat([Adult,dummy],axis=1)\n",
    "Adult = Adult.drop(columns=['maritalstatus','occupation','sex','nativecountry'])\n",
    "print(Adult.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating x an y columns\n",
    "\n",
    "y_column = 'capitalgain'\n",
    "x_columns = [x for x in Adult.columns if x not in y_column]\n",
    "\n",
    "y = Adult[y_column]\n",
    "x = Adult[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Fit the data(train the model)\n",
    "\n",
    "regression_model.fit(x, y)\n",
    "\n",
    "# Predict\n",
    "\n",
    "y_predicted = regression_model.predict(x)\n",
    "\n",
    "#Summary Statistics\n",
    "\n",
    "X = sms.add_constant(x)\n",
    "\n",
    "# Note the diference in argument order\n",
    "\n",
    "model = sms.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    The model is not very accurate, even with the increased number of features. The R2 value is at .30 and the adjusted R2 value is at .28 which both are higher than the previous model which had R2 and adjusted R2 values of .23. With the large amount of features I am worried about overfitting, so I will run backwards selection until I have the least amount of features with the highest adjusted R2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68 features - removing the first one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 68) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 68\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    Removing one feature did not affect the R2 score but did slightly increase the Adj R2 score. Which means that it reduced the issue of overfitting. I will now continue with removing an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67 features - removing the second one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 67) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 67\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    Removing another feature also slightly increased adjusted R2. I will continue with the backwards selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 features - removing the second one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 66) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 66\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    Adjusted Rsquared continues to increase. I will continue with backwards selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 65 features - removing the second one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 65) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 65\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    Adjusted Rsquared continues to increase. I will continue with backwards selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64 features - removing the second one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 64) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 64\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    Adjusted Rsquared continues to increase. I will continue with backwards selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63 features - removing the second one\n",
    "\n",
    "# Backwards Selection\n",
    "\n",
    "model = LinearRegression() # use linear regression model for all features\n",
    "rfe = RFE(model, 63) # use rfe\n",
    "fit = rfe.fit(x, y) # fit our model\n",
    "# Predict\n",
    "\n",
    "y_predicted = fit.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "n = 32561\n",
    "p = 63\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "print('Adj R2 score',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    Adjusted Rsquared decreased, which means that I will use a final Multiple Linear Regression model with 64 features. The features that were removed were [maritalstatus_ Married-spouse-absent, maritalstatus_ Separated, nativecountry_ Philippines, nativecountry_ Laos, nativecountry_ Jamaica]. However, even with adding additional features to this model the R2 and adjusted R2 values are not very high. The variables in my model do not appear to have much sigificant explanatory power for capital-gain. I will next explore whether or not running a PCR model in the presence of additional features will have any sigificant uplift in R2/adjusted R2. However, if that method does not prove to work, I will see if the features are better suited explaining whether or not someone makes >50k annually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping features eliminated in backwards selection\n",
    "\n",
    "Adult = Adult.drop(columns=['maritalstatus_ Married-spouse-absent', 'maritalstatus_ Separated', \n",
    "                             'nativecountry_ Philippines', 'nativecountry_ Laos', 'nativecountry_ Jamaica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'capitalgain'\n",
    "feature_labels = [x for x in Adult.columns if x not in [target_label]]\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "Adult_expanded = pd.get_dummies(Adult, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(Adult_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "y = Adult[target_label]\n",
    "x = Adult.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all columns\n",
    "\n",
    "# Create x-scaled\n",
    "\n",
    "x_scaled = np.apply_along_axis(scale, 0, x)\n",
    "print(x_scaled)\n",
    "\n",
    "# Create a scaled y-target.\n",
    "\n",
    "y_scaled = np.apply_along_axis(scale, 0, y)\n",
    "print(y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "pca_result = pca.fit_transform(x_scaled)\n",
    "column_names = ['pc' + str(ix+1) for ix in range(x_scaled.shape[1])]\n",
    "pca_df = pd.DataFrame(data = pca_result, columns=column_names)\n",
    "pca_df[target_label] = y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance for all principal components.\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Explained variance by Principal Component Num')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The first 7 components appear to explain the majority of the variance, then there is a flat drop off of equivalent compentnets. The last 10 compenents do not explain much variance. Since the first 7 compenents appear to explore the majority of the variance, I will run a PCR with only the first 7 compenents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 components\n",
    "\n",
    "# Perform linear regression with the first N columns.\n",
    "\n",
    "n = 7\n",
    "formula_start = target_label + ' ~ '\n",
    "formula_terms = ['pc' + str(x+1) for x in range(n)]\n",
    "formula_end = ' + '.join(formula_terms)\n",
    "formula_final = formula_start + formula_end\n",
    "pcr_model = sm.ols(formula = formula_final, data=pca_df)\n",
    "results = pcr_model.fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The adjusted R-squared for the PCR model with 7 components is at .026 which is slightly less than the Multiple Linear Regression model. To ensure there is no underfitting I will run the model with 8 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 components\n",
    "\n",
    "# Perform linear regression with the first N columns.\n",
    "\n",
    "n = 8\n",
    "formula_start = target_label + ' ~ '\n",
    "formula_terms = ['pc' + str(x+1) for x in range(n)]\n",
    "formula_end = ' + '.join(formula_terms)\n",
    "formula_final = formula_start + formula_end\n",
    "pcr_model = sm.ols(formula = formula_final, data=pca_df)\n",
    "results = pcr_model.fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    With 8 compenents the adjusted R2 score increases from .25 to .26. These implies that the previous model was underfitted using only the first 7 compenents. I will now test with 9 compenents to see if there is still underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 components\n",
    "\n",
    "# Perform linear regression with the first N columns.\n",
    "\n",
    "n = 9\n",
    "formula_start = target_label + ' ~ '\n",
    "formula_terms = ['pc' + str(x+1) for x in range(n)]\n",
    "formula_end = ' + '.join(formula_terms)\n",
    "formula_final = formula_start + formula_end\n",
    "pcr_model = sm.ols(formula = formula_final, data=pca_df)\n",
    "results = pcr_model.fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "    \n",
    "    With 9 compenents the adjusted R2 score goes back down to .25, which means that 8 components is the proper amount of components for this model. However, this score is lower than even the multiple linear regression model and we can infer that these variables are not good explainers of captial gain. I will now run a similar analyis using Naive Bayes but seeing if they are a good explaination of whether or not someone makes >50k anually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading url\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "Adult= pd.read_csv(url, header=None)\n",
    "\n",
    "# Assigning reasonable column names\n",
    "\n",
    "Adult.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"educationnum\",\"maritalstatus\",\"occupation\",\n",
    "                 \"relationship\",\"race\",\"sex\",\"capitalgain\",\"capitalloss\",\"hoursperweek\",\"nativecountry\",\"k\"]\n",
    "print(Adult.columns)\n",
    "Adult.describe()\n",
    "\n",
    "#Removing cases with missing data\n",
    "\n",
    "Adult = Adult.replace(to_replace= \"?\", value=float(\"NaN\"))\n",
    "Adult_null = Adult.isnull().sum()\n",
    "print(Adult_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Hot Encode Categorical Data\n",
    "\n",
    "onehot_features = ['k']\n",
    "dummy = pd.get_dummies(Adult[onehot_features])\n",
    "dummy.head()\n",
    "Adult = pd.concat([Adult,dummy],axis=1)\n",
    "Adult = Adult.drop(columns=['k','k_ <=50K'])\n",
    "print(Adult.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'k_ >50K'\n",
    "non_features = [\"workclass\",\"fnlwgt\",\"education\",\"maritalstatus\",\"occupation\",\n",
    "                 \"relationship\",\"race\",\"sex\",\"capitalloss\",\"nativecountry\",\"capitalgain\"]\n",
    "feature_labels = [x for x in Adult.columns if x not in [target_label] + non_features]\n",
    "\n",
    "# Filter out non-features and non-targets\n",
    "\n",
    "Adult = Adult.drop(non_features, axis=1)\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "Adult_expanded = pd.get_dummies(Adult, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(Adult_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "y = Adult[target_label]\n",
    "x = Adult.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adult.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the model on the training sets only\n",
    "\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#print(\"R2 score:\",metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy: 10-fold cross-validation\n",
    "\n",
    "ten_fold_scores = cross_val_score(GaussianNB(), x, y, scoring='accuracy', cv=10)\n",
    "print (ten_fold_scores)\n",
    "print (\"The mean of the 10 fold cross validation scores is:\", ten_fold_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The accuracy score for this model is around 80% (which is fairly strong) and the 10 fold cross validation test has similar results. This model appears to be more accurate then the Multiple Linear Regression and PCR models ran trying to see if demographic variables can explain capital gain. This implies that age, education-num, and hours-per-week are fairly accurate predictors of whether or not an individual makes >50k. I ran a similar Naive Bayes model in assignment 4 which instead had capital gain as the target, that model had an accuracy score much lower (around .20). This is further evidence that other variables in this dataset are not strong predictors of capital gain. However, age, education-num, and hours-per-week are fairly strong indicators of whether or not someone makes >50k income.\n",
    "    \n",
    "    I conclude that:\n",
    "    \n",
    "    (1) age, education-num, and hours-per-week are fairly accurate indicators of whether or not someone makes >50k income, in this dataset\n",
    "    \n",
    "    (2) There are not strong demographic predictors in this dataset of capital gain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
