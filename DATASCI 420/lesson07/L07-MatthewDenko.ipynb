{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 07 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "    Kennedy's oceanographic institute client pulled into port the other day with a ton (literally) of collected samples and corresponding data to process. Some of these data tasks are being distributed to others to work on; you've got the abalone (marine snails) data to classify and determine the age from physical characteristics. \n",
    "\n",
    "    Age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope. Other measurements, which are easier to obtain, could be used to predict the age. According to the data provider, original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled (by dividing by 200) for use with machine learning algorithms such as SVMs.\n",
    "\n",
    "    The target field is “Rings”. Since the output is continuous the solution can be handled by a Support Vector Regression or it can be changed to a binary Support Vector Classification by assigning examples that are younger than 11 years old to class: ‘0’ and those that are older (class: ‘1’)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "    It is recommended you complete the lab exercises for this lesson before beginning the assignment.\n",
    "\n",
    "    Using the Abalone csv file,  create a new notebook to build an experiment using support vector machine classifier and regression. Perform each of the following tasks and answer the questions:\n",
    "\n",
    "    (1) Convert the continuous output value from continuous to binary (0,1) and build an SVC\n",
    "    (2) Using your best guess for hyperparameters and kernel, what is the percentage of correctly classified results?\n",
    "    (3) Test different kernels and hyperparameters or consider using sklearn.model_selection.SearchGridCV. Which kernel performed best with what settings?\n",
    "    (4) Show recall, precision and f-measure for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Convert the continuous output value from continuous to binary (0,1) and build an SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading url\n",
    "\n",
    "data = pd.read_csv(\"/Users/matt.denko/Downloads/Abalone.csv\") \n",
    "data.columns = ['Sex',\n",
    "'Length',\n",
    "'Diameter',\n",
    "'Height',\n",
    "'Whole Weight',\n",
    "'Shucked Weight',\n",
    "'Viscera Weight',\n",
    "'Shell Weight',\n",
    "'Rings'] \n",
    "(nrows, ncols) = data.shape\n",
    "print(data.columns)\n",
    "data.describe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cases with missing data\n",
    "\n",
    "data = data.replace(to_replace= \"?\", value=float(\"NaN\"))\n",
    "data_null = data.isnull().sum()\n",
    "print(data_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'Rings'\n",
    "feature_labels = [x for x in data.columns if x not in [target_label]]\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "y = data[target_label]\n",
    "x = data.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "# Convert data to Binary\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test\n",
    "\n",
    "le =  ce.OneHotEncoder(return_df=False, handle_missing=\"ignore\", handle_unknown=\"ignore\")\n",
    "\n",
    "# Fit Model\n",
    "\n",
    "le.fit(X_train)\n",
    "X_encoded_train = le.transform(X_train)\n",
    "X_encoded_test = le.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Using your best guess for hyperparameters and kernel, what is the percentage of correctly classified results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "\n",
    "cost = .9 # penalty parameter of the error term\n",
    "gamma = 5 # defines the influence of input vectors on the margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a LinearSVC\n",
    "\n",
    "clf1 = svm.LinearSVC(C=cost).fit(X_encoded_train, Y_train)\n",
    "clf1.predict(X_encoded_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(clf1.predict(X_encoded_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Test different kernels and hyperparameters or consider using sklearn.model_selection.SearchGridCV. Which kernel performed best with what settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test linear, rbf and poly kernels\n",
    "\n",
    "for k in ('linear', 'rbf', 'poly'):\n",
    "    clf = svm.SVC(gamma=gamma, kernel=k, C=cost).fit(X_encoded_train, Y_train)\n",
    "    clf.predict(X_encoded_test)\n",
    "    print(k)\n",
    "    print(classification_report(clf.predict(X_encoded_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make plotting easier, let's just use two features.\n",
    "\n",
    "X = data.loc[:,('Height','Length')]\n",
    "Y = y\n",
    "\n",
    "h = .5  # step size in the mesh\n",
    "cost = .9  # cost\n",
    "gamma = 10 # gamma \n",
    "\n",
    "# testing other kernels on unscaled data (for plotting tht support vectors)\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=cost).fit(X, Y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=gamma, C=cost).fit(X, Y)\n",
    "poly_svc = svm.SVC(kernel='poly', gamma=gamma, degree=3, C=cost).fit(X, Y)\n",
    "lin_svc = svm.LinearSVC(C=cost).fit(X, Y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "\n",
    "x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
    "y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial kernel',\n",
    "          'LinearSVC (linear kernel)']\n",
    "\n",
    "for i, kernel in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    Z = kernel.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The kernal that performed the best was the polynomial kernal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Show recall, precision and f-measure for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test linear, rbf and poly kernels\n",
    "\n",
    "for k in ('linear', 'rbf', 'poly'):\n",
    "    clf = svm.SVC(gamma=gamma, kernel=k, C=cost).fit(X_encoded_train, Y_train)\n",
    "    clf.predict(X_encoded_test)\n",
    "    print(k)\n",
    "    print(classification_report(clf.predict(X_encoded_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    The kernal that performed the best was the polynomial kernal. It had a F-measure, recall, and precision micro average of .29. While the linear and rbf had lower precision, recall and F-measure scores."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
