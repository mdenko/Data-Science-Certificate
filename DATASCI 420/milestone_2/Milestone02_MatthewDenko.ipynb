{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "    The capstone project focuses on diaper manufacturing quality. In the Lesson 01 assignment, you discovered how the diaper manufacturing process works. Generally, to ensure or predict quality, a diaper manufacturer need s to monitor every step of the manufacturing process with sensors such as heat sensors, glue sensors, glue level, etc.\n",
    "\n",
    "    For this capstone project, we will use the SECOM manufacturing Data Set from the UCI Machine Learning Repository. The set is originally for semiconductor manufacturing, but in our case, we will assume that it is for the diaper manufacturing process.\n",
    "\n",
    "    The dataset consists of two files:\n",
    "\n",
    "    (1) a dataset file SECOM containing 1567 examples, each with 591 features, presented in a 1567 x 591 matrix\n",
    "    (2) a labels file listing the classifications and date time stamp for each example\n",
    "    \n",
    "    Reference\n",
    "    Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "    Create a new notebook for this assignment named Milestone02_studentname.ipynb (replacing studentname with your own).\n",
    "\n",
    "    (1) Split prepared data from Milestone 1 into training and testing\n",
    "    (2) Build a decision tree model that detects faulty products\n",
    "    (3) Build an ensemble model that detects faulty products\n",
    "    (4) Build an SVM model\n",
    "    (5) Evaluate all three models\n",
    "    (6) Describe your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Split prepared data from Milestone 1 into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree \n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6',\n",
      "       'feature7', 'feature8', 'feature9', 'feature10',\n",
      "       ...\n",
      "       'feature583', 'feature584', 'feature585', 'feature586', 'feature587',\n",
      "       'feature588', 'feature589', 'feature590', 'classification', 'date'],\n",
      "      dtype='object', length=592)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "secom_var = pd.read_csv(url, sep=\" \", names=names, na_values = \"NaN\") \n",
    " \n",
    "url_l = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "secom_labels = pd.read_csv(url_l,sep=\" \",names = [\"classification\",\"date\"],parse_dates = [\"date\"],na_values = \"NaN\")\n",
    "feature_names = secom_labels\n",
    "\n",
    "#Merging the two datasets\n",
    "\n",
    "data = pd.merge(secom_var, secom_labels,left_index=True,right_index=True)\n",
    "data.describe()\n",
    "data.head()\n",
    "print(data.columns) \n",
    "(nrows, ncols) = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1          0\n",
      "feature2          0\n",
      "feature3          0\n",
      "feature4          0\n",
      "feature5          0\n",
      "feature6          0\n",
      "feature7          0\n",
      "feature8          0\n",
      "feature9          0\n",
      "feature10         0\n",
      "feature11         0\n",
      "feature12         0\n",
      "feature13         0\n",
      "feature14         0\n",
      "feature15         0\n",
      "feature16         0\n",
      "feature17         0\n",
      "feature18         0\n",
      "feature19         0\n",
      "feature20         0\n",
      "feature21         0\n",
      "feature22         0\n",
      "feature23         0\n",
      "feature24         0\n",
      "feature25         0\n",
      "feature26         0\n",
      "feature27         0\n",
      "feature28         0\n",
      "feature29         0\n",
      "feature30         0\n",
      "                 ..\n",
      "feature563        0\n",
      "feature564        0\n",
      "feature565        0\n",
      "feature566        0\n",
      "feature567        0\n",
      "feature568        0\n",
      "feature569        0\n",
      "feature570        0\n",
      "feature571        0\n",
      "feature572        0\n",
      "feature573        0\n",
      "feature574        0\n",
      "feature575        0\n",
      "feature576        0\n",
      "feature577        0\n",
      "feature578        0\n",
      "feature579        0\n",
      "feature580        0\n",
      "feature581        0\n",
      "feature582        0\n",
      "feature583        0\n",
      "feature584        0\n",
      "feature585        0\n",
      "feature586        0\n",
      "feature587        0\n",
      "feature588        0\n",
      "feature589        0\n",
      "feature590        0\n",
      "classification    0\n",
      "date              0\n",
      "Length: 592, dtype: int64\n",
      "There are 0 columns with missing data\n"
     ]
    }
   ],
   "source": [
    "# Replacing Nulls\n",
    "\n",
    "data = data.replace(to_replace= float('NaN'), value=float(0))\n",
    "data_null = data.isnull().sum()\n",
    "print(data_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "    I replaced the null values with a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame one-hot-expanded shape: (1567, 592)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'classification'\n",
    "non_feature = 'date'\n",
    "feature_labels = [x for x in data.columns if x not in [target_label]+ [non_feature]]\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "data_expanded = pd.get_dummies(data, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(data_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "y = data[target_label]\n",
    "x = data.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build a decision tree model that detects faulty products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.71153837e-03 5.24252587e-06 3.84566463e-03 8.51207610e-04\n",
      " 2.41536189e-03]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "nTrees = 100\n",
    "max_depth = 5\n",
    "min_node_size = 5\n",
    "verbose = 0\n",
    "learning_rate = 0.05\n",
    "gbm_clf = GradientBoostingClassifier(n_estimators=nTrees, loss='deviance', learning_rate=learning_rate, max_depth=max_depth, \\\n",
    "                                   min_samples_leaf=min_node_size)\n",
    "gbm_clf.fit(X_train, y_train)\n",
    "print(gbm_clf.feature_importances_[:5])\n",
    "dt_y_test_hat = gbm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Build an ensemble model that detects faulty products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00556305 0.         0.0035221  0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "nTrees = 100\n",
    "max_depth = 5\n",
    "min_node_size = 5\n",
    "verbose = 0\n",
    "clf = RandomForestClassifier(n_estimators=nTrees, max_depth=max_depth, random_state=0, verbose=verbose, min_samples_leaf=min_node_size)\n",
    "clf.fit(X_train, y_train)\n",
    "clf2 = clf.fit(X_train, y_train)\n",
    "print(clf.feature_importances_[:5])\n",
    "rf_y_test_hat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Build an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "\n",
    "cost = .9 # penalty parameter of the error term\n",
    "gamma = 5 # defines the influence of input vectors on the margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.93      0.96       470\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       471\n",
      "   macro avg       0.50      0.47      0.48       471\n",
      "weighted avg       1.00      0.93      0.96       471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Test a LinearSVC\n",
    "\n",
    "clf1 = svm.LinearSVC(C=cost).fit(X_train, y_train)\n",
    "clf1.predict(X_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(clf1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Evaluate all three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9341825902335457\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "dt_accuracy_score = accuracy_score(y_test, dt_y_test_hat)\n",
    "print(dt_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9341825902335457\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "rf_accuracy_score = accuracy_score(y_test, rf_y_test_hat)\n",
    "print(rf_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.07      0.92      0.14        36\n",
      "           1       0.90      0.06      0.12       435\n",
      "\n",
      "   micro avg       0.13      0.13      0.13       471\n",
      "   macro avg       0.49      0.49      0.13       471\n",
      "weighted avg       0.84      0.13      0.12       471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Test a LinearSVC\n",
    "\n",
    "clf1 = svm.LinearSVC(C=cost).fit(X_train, y_train)\n",
    "clf1.predict(X_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(clf1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Describe your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "    In Milestone 1, I originally ran a model using all 591 features to predict whether or not a quality diaper was produced. This model had very low accuracy. To improve the model, I then used a SMOTE method to handle class imbalance, this improved the accuracy significantly however it was still only .57. I then used recursive feature selection to handle the issue of overfitting. This improved the accuracy to .932. In this Milestone 2 assignment, I first ran a decision tree model which produced an accuracy of .932. I then ran a random forest model which produced an accuracy of .934 which is the highest accuracy that I have gottent in predicting whether or not that a quality diaper was produced."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
