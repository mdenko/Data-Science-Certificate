{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 08 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workplace Scenario\n",
    "    \n",
    "    Mackenzie has a wine bottling factory and recently attended a followup symposium on deep learning and has returned with fascinating ideas to build neural networks to perform classification tasks. She had the idea of building a red-white wine classifier so that the bottle labels can be automatically generated. She approached your team, and suggested that you build a red-white wine classifier as a modified simple perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "    \n",
    "    It is recommended you complete the lab exercises for this lesson before beginning the assignment.\n",
    "\n",
    "    For this assignment you will start from the perceptron neural network notebook (Simple Perceptron Neural Network.ipynb) and modify the python code to make it into a multi-layer neural network classifier. To test your system, use the RedWhiteWine.csv file with the goal of building a red or white wine classifier. Use all the features in the dataset, allowing the network to decide how to build the internal weighting system. To review the data attributes, download the L08_WineQuality.pdf.  Perform each of the following tasks and answer the related questions:\n",
    "\n",
    "    (1) Use the provided RedWhiteWine.csv file. Include ALL the features with “Class” being your output vector\n",
    "    (2) Use the provided Simple Perceptron Neural Network notebook to develop a multi-layer feed-forward/backpropagation neural network\n",
    "    (3) Be able to adjust the following between experiments:\n",
    "        -Learning Rate\n",
    "        -Number of epochs\n",
    "        -Depth of architecture—number of hidden layers between the input and output layers\n",
    "        -Number of nodes in a hidden layer—width of the hidden layers\n",
    "    (4) Determine what the best neural network structure and hyperparameter settings results in the best predictive capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Use the provided RedWhiteWine.csv file. Include ALL the features with “Class” being your output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree \n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "\n",
    "data = pd.read_csv(\"/Users/matt.denko/Downloads/RedWhiteWine.csv\") \n",
    "data.columns = [\n",
    "'fixed acidity',\n",
    "'volatile acidity',\n",
    "'citric acid',\n",
    "'residual sugar',\n",
    "'chlorides',\n",
    "'free sulfur dioxide',\n",
    "'total sulfur dioxide',\n",
    "'density',\n",
    "'pH',\n",
    "'sulphates',\n",
    "'alcohol',\n",
    "'quality',\n",
    "'Class'] \n",
    "(nrows, ncols) = data.shape\n",
    "print(data.columns)\n",
    "data.describe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'Class'\n",
    "feature_labels = [x for x in data.columns if x not in [target_label]]\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "data_expanded = pd.get_dummies(data, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(data_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "Y = data[target_label]\n",
    "X = data.as_matrix(columns=feature_labels)\n",
    "\n",
    "# Create an artificial dataset\n",
    "x1 = np.array(np.arange(0.1,0.7,0.1))\n",
    "X1 = np.exp(x1 * 1.1 + 0.75)\n",
    "x2 = np.array(np.arange(0.6,1.2,0.1))\n",
    "X2 = np.exp(x2 * 0.4 + 0.75)\n",
    "\n",
    "#From the output, lets use 3 as threshold; Value>3 = class 1, value<3 = class 0\n",
    "X = np.array([X1,X2])\n",
    "Y = np.array([0,0,0,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (Sigmoid) Function\n",
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low. Additionally, I am using logistic (sigmoid) function $\\frac{1}{1+e^-z}$. This can also be expressed as $\\frac{e^z}{1+e^z}$. NOTE: you could call sklearn.linear_model.LogisticRegressionCV(), but it's always good to try and write it yourself so you understand what the function does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a numerically stable logistic s-shaped definition to call\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Because this there are not hidden layers, the second dimension is always assigned to 1. std is set to ${1^{-1}}$ to ensure values are between zero and 1. If zeros, there's no reason to multiply with std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimentions and set the weights to random numbers\n",
    "def init_parameters(dim1, dim2=1,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Use the provided Simple Perceptron Neural Network notebook to develop a multi-layer feed-forward/backpropagation neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer network: Forward Prop\n",
    "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
    "def fwd_prop(W1,bias,X,Y):\n",
    "\n",
    "    Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
    "    A1 = sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
    "\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am calculating the loss/cost. The loss function here is a logistic loss function and in this case of binary classification, this is also a cross-entropy loss\n",
    "\n",
    "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
    "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $\n",
    "\n",
    "Lastly, the gradients W1 and B1 are calculated and returned along with the total cost/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer network: Backprop\n",
    "\n",
    "def back_prop(A1,W1,bias,X,Y):\n",
    "\n",
    "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    # Cross entropy loss function\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
    "    dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
    "    \n",
    "    grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
    "    \n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Be able to adjust the following between experiments:\n",
    "### -Learning Rate\n",
    "### -Number of epochs\n",
    "### -Depth of architecture—number of hidden layers between the input and output layers\n",
    "### -Number of nodes in a hidden layer—width of the hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final parameters W1 (updated weight vector), B1 (bias) and the loss array after running for given number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
    "    \n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, True)\n",
    "    B1 = init_parameters(n_1,1, True)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = fwd_prop(W1,B1,X,Y)                # get predicted vector\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)    # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "        \n",
    "        loss_array[i] = cost                    # loss array gets cross ent values\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1}           # assign \n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "Now that all of the helper functions are created we can run gradient descent on the handcrafted dataset that I had created earlier. Note that I am using n_1 = 1, therefore, I am just using one output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,n_1= 1 )\n",
    "print(loss_array[num_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the boundary of separation is 0. That is values less than 0 belong to class 0 and greater than 0 belong to class 1.\n",
    "Key thing to note here is that the data we generated was a linearly separable data and hence there are many possible options for the separting plane. Unlike SVM, logistic regression does not necessarily find the best separting plane, but finds a locally optimum solution that separates the classes of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the loss array\n",
    "Here we want to ensure that the loss value per iteration is going down. However, note that the plot has not curved to reach stablizing value i.e we can run the algorithms more times to get a lower loss. However, this is not needed as the current value of the parameters can classify the input data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Determine what the best neural network structure and hyperparameter settings results in the best predictive capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with handcrafted dataset\n",
    "values below 0.5 are assigned to class 1 and values above are set to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.25,0.75],[0.1,0.9],[0.3,0.8],[0.8,0.25],[0.9,0.2],[0.7,0.1]])\n",
    "X = np.array([[0.11,0.12],[0.05,0.1],[0.15,0.11],[0.8,0.9],[0.9,0.8],[0.85,0.95]])\n",
    "X = X.T #Had to do this because, I did not declare the X array as (#dimension * # Datapoints)\n",
    "Y = np.array([1,1,1,0,0,0])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loss_array = run_grad_desc(100000,0.01,X,Y,n_1= 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
