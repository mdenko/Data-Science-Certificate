{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "    The capstone project focuses on diaper manufacturing quality. In the Lesson 01 assignment, you discovered how the diaper manufacturing process works. Generally, to ensure or predict quality, a diaper manufacturer need s to monitor every step of the manufacturing process with sensors such as heat sensors, glue sensors, glue level, etc.\n",
    "\n",
    "    For this capstone project, we will use the SECOM manufacturing Data Set from the UCI Machine Learning Repository. The set is originally for semiconductor manufacturing, but in our case, we will assume that it is for the diaper manufacturing process.\n",
    "\n",
    "    The dataset consists of two files:\n",
    "\n",
    "    (1) a dataset file SECOM containing 1567 examples, each with 591 features, presented in a 1567 x 591 matrix\n",
    "    (2) a labels file listing the classifications and date time stamp for each example\n",
    "    \n",
    "    Reference\n",
    "    Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "    Create a new notebook and perform each of the following tasks and answer the related questions:\n",
    "\n",
    "    (1) Build a simple neural networks model\n",
    "    (2) Build a DNN model\n",
    "    (3) Build a RNN model\n",
    "    (4) Summarize your findings with examples.  Explain what the manufacturer should focus on to optimize the diaper manufacturing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Build a simple neural networks model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "    For my simple neural networks model I will be building a Keras model with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import reuters\n",
    "\n",
    "#import numpy and matplotlib\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6',\n",
      "       'feature7', 'feature8', 'feature9', 'feature10',\n",
      "       ...\n",
      "       'feature583', 'feature584', 'feature585', 'feature586', 'feature587',\n",
      "       'feature588', 'feature589', 'feature590', 'classification', 'date'],\n",
      "      dtype='object', length=592)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "secom_var = pd.read_csv(url, sep=\" \", names=names, na_values = \"NaN\") \n",
    " \n",
    "url_l = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "secom_labels = pd.read_csv(url_l,sep=\" \",names = [\"classification\",\"date\"],parse_dates = [\"date\"],na_values = \"NaN\")\n",
    "feature_names = secom_labels\n",
    "\n",
    "#Merging the two datasets\n",
    "\n",
    "data = pd.merge(secom_var, secom_labels,left_index=True,right_index=True)\n",
    "data.describe()\n",
    "data.head()\n",
    "print(data.columns) \n",
    "(nrows, ncols) = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1          0\n",
      "feature2          0\n",
      "feature3          0\n",
      "feature4          0\n",
      "feature5          0\n",
      "feature6          0\n",
      "feature7          0\n",
      "feature8          0\n",
      "feature9          0\n",
      "feature10         0\n",
      "feature11         0\n",
      "feature12         0\n",
      "feature13         0\n",
      "feature14         0\n",
      "feature15         0\n",
      "feature16         0\n",
      "feature17         0\n",
      "feature18         0\n",
      "feature19         0\n",
      "feature20         0\n",
      "feature21         0\n",
      "feature22         0\n",
      "feature23         0\n",
      "feature24         0\n",
      "feature25         0\n",
      "feature26         0\n",
      "feature27         0\n",
      "feature28         0\n",
      "feature29         0\n",
      "feature30         0\n",
      "                 ..\n",
      "feature563        0\n",
      "feature564        0\n",
      "feature565        0\n",
      "feature566        0\n",
      "feature567        0\n",
      "feature568        0\n",
      "feature569        0\n",
      "feature570        0\n",
      "feature571        0\n",
      "feature572        0\n",
      "feature573        0\n",
      "feature574        0\n",
      "feature575        0\n",
      "feature576        0\n",
      "feature577        0\n",
      "feature578        0\n",
      "feature579        0\n",
      "feature580        0\n",
      "feature581        0\n",
      "feature582        0\n",
      "feature583        0\n",
      "feature584        0\n",
      "feature585        0\n",
      "feature586        0\n",
      "feature587        0\n",
      "feature588        0\n",
      "feature589        0\n",
      "feature590        0\n",
      "classification    0\n",
      "date              0\n",
      "Length: 592, dtype: int64\n",
      "There are 0 columns with missing data\n"
     ]
    }
   ],
   "source": [
    "# Replacing Nulls\n",
    "\n",
    "data = data.replace(to_replace= -1, value=float(0))\n",
    "data = data.replace(to_replace= float('NaN'), value=float(0))\n",
    "data_null = data.isnull().sum()\n",
    "print(data_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame one-hot-expanded shape: (1567, 592)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'classification'\n",
    "non_feature = 'date'\n",
    "feature_labels = [x for x in data.columns if x not in [target_label]+ [non_feature]]\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "data_expanded = pd.get_dummies(data, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(data_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "Y = data[target_label]\n",
    "X = data.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Model\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(590,)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.relu)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1096/1096 [==============================] - 0s 227us/step - loss: 4.4015 - acc: 0.7062\n",
      "Epoch 2/5\n",
      "1096/1096 [==============================] - 0s 36us/step - loss: 2.8613 - acc: 0.8130\n",
      "Epoch 3/5\n",
      "1096/1096 [==============================] - 0s 38us/step - loss: 2.4415 - acc: 0.8412\n",
      "Epoch 4/5\n",
      "1096/1096 [==============================] - 0s 40us/step - loss: 1.3354 - acc: 0.9170\n",
      "Epoch 5/5\n",
      "1096/1096 [==============================] - 0s 41us/step - loss: 1.2772 - acc: 0.9206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a21985160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile Model\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471/471 [==============================] - 0s 102us/step\n",
      "Test accuracy: 0.925690021737619\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Accuracy Score\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1096/1096 [==============================] - 1s 848us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 2/10\n",
      "1096/1096 [==============================] - 0s 241us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 3/10\n",
      "1096/1096 [==============================] - 0s 238us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 4/10\n",
      "1096/1096 [==============================] - 0s 236us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 5/10\n",
      "1096/1096 [==============================] - 0s 230us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 6/10\n",
      "1096/1096 [==============================] - 0s 237us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 7/10\n",
      "1096/1096 [==============================] - 0s 234us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 8/10\n",
      "1096/1096 [==============================] - 0s 255us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 9/10\n",
      "1096/1096 [==============================] - 0s 241us/step - loss: 14.8805 - acc: 0.0666\n",
      "Epoch 10/10\n",
      "1096/1096 [==============================] - 0s 247us/step - loss: 14.8805 - acc: 0.0666\n",
      "471/471 [==============================] - 0s 202us/step\n",
      "Test accuracy: 0.06581740980600095\n"
     ]
    }
   ],
   "source": [
    "# 1 - Adding More layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(590,)),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Build a RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 590, 46)           460000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               58800     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4579)              462479    \n",
      "=================================================================\n",
      "Total params: 981,279\n",
      "Trainable params: 981,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt.denko/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1096 samples, validate on 471 samples\n",
      "Epoch 1/3\n",
      "1096/1096 [==============================] - 18s 17ms/step - loss: 6.7460 - acc: 0.3613 - val_loss: 0.3918 - val_acc: 0.9342\n",
      "Epoch 2/3\n",
      "1096/1096 [==============================] - 18s 16ms/step - loss: 0.3198 - acc: 0.9334 - val_loss: 0.3068 - val_acc: 0.9342\n",
      "Epoch 3/3\n",
      "1096/1096 [==============================] - 20s 18ms/step - loss: 0.3353 - acc: 0.9334 - val_loss: 0.3392 - val_acc: 0.9342\n",
      "471/471 [==============================] - 2s 5ms/step\n",
      "Test accuracy: 0.9341825907397422\n"
     ]
    }
   ],
   "source": [
    "# Construct model 3\n",
    "\n",
    "X_train = np.clip(X_train, 0,9999)\n",
    "X_test = np.clip(X_test, 0, 9999)\n",
    "\n",
    "\n",
    "max_review_length = 590 \n",
    "num_of_words = 10000\n",
    "embedding_vecor_length = 46\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(4579, activation='relu'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Summarize your findings with examples.  Explain what the manufacturer should focus on to optimize the diaper manufacturing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "    In Milestone 1, I originally ran a model using all 591 features to predict whether or not a quality diaper was produced. This model had very low accuracy. To improve the model, I then used a SMOTE method to handle class imbalance, this improved the accuracy significantly however it was still only .57. I then used recursive feature selection to handle the issue of overfitting. This improved the accuracy to .932. Then in the Milestone 2 assignment, I first ran a decision tree model which produced an accuracy of .932. I then ran a random forest model which produced an accuracy of .934 which is the highest accuracy that I have gottent in predicting whether or not that a quality diaper was produced. For this final Milestone assignment I then ran three different neural networks model. The first model that I ran was a simple neural networks model which was a keras model with a binary crossentropy loss function and one hidden layer. This model produced an accuracy of 81%. The second model that I ran was a similar dnn neural networks model with a binary crossentropy loss function, however I added on additional hidden layers. The addition of hidden layers actually made the accuracy of the model decrease and produced a score of 7%. I then ran a RNN neural networks model using a sparse_categorical_crossentropy loss function which produced the highest accuracy of any of the models with 93.4% accuracy. Based on the results, my reccomendation to the diaper manufacturer would be to not invest in leveraging neural networks to optimize the manufacturing process. The accuracy score of the best neural networks model was comparable to the accuracy score of a random forest model. The neural networks model will be more expensive and harder to act on then a random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
