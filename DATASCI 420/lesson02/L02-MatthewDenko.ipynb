{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 02 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "    You are involved in a project where you are tasked to build a machine learning algorithm that distinguishes between \"bad'' connections (called intrusions or attacks) and \"good'' (normal) connections. Note that the number of normal connections is greater than that of bad ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "    (1) Read data\n",
    "    (2) Build a classifier\n",
    "    (3) Determine your model accuracy\n",
    "    (4) Modify data by handling class imbalance\n",
    "    (5) Use the same mode on updated data\n",
    "    (6) What is the accuracy?\n",
    "    (7) Describe your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading url\n",
    "\n",
    "data = pd.read_csv(\"/Users/matt.denko/Downloads/Intrusion Detection.csv\") \n",
    "data.columns = [\"duration\",\n",
    "\"protocol_type\",\n",
    "\"service\",\n",
    "\"flag\",\n",
    "\"src_bytes\",\n",
    "\"dst_bytes\",\n",
    "\"land\",\n",
    "\"wrong_fragment\",\n",
    "\"urgent\",\n",
    "\"hot\",\n",
    "\"num_failed_logins\",\n",
    "\"logged_in\",\n",
    "\"num_compromised\",\n",
    "\"root_shell\",\n",
    "\"su_attempted\",\n",
    "\"num_root\",\n",
    "\"num_file_creations\",\n",
    "\"num_shells\",\n",
    "\"num_access_files\",\n",
    "\"num_outbound_cmds\",\n",
    "\"is_host_login\",\n",
    "\"is_guest_login\",\n",
    "\"count\",\n",
    "\"srv_count\",\n",
    "\"serror_rate\",\n",
    "\"srv_serror_rate\",\n",
    "\"rerror_rate\",\n",
    "\"srv_rerror_rate\",\n",
    "\"same_srv_rate\",\n",
    "\"diff_srv_rate\",\n",
    "\"srv_diff_host_rate\",\n",
    "\"dst_host_count\",\n",
    "\"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\",\n",
    "\"dst_host_diff_srv_rate\",\n",
    "\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\n",
    "\"dst_host_serror_rate\",\n",
    "\"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\",\n",
    "\"dst_host_srv_rerror_rate\",\n",
    "\"connection_type\"]\n",
    "print(data.columns)\n",
    "data.describe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cases with missing data\n",
    "\n",
    "data = data.replace(to_replace= \"?\", value=float(\"NaN\"))\n",
    "data_null = data.isnull().sum()\n",
    "print(data_null)\n",
    "print(\"There are 0 columns with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "    I am going to build a classier predicting whether or not a same host connection has a greater percentage of SYN errors than the average. To do this, I have to first determine the average then create a dummy variable for the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the mean\n",
    "\n",
    "serror_rate = data.loc[:,\"serror_rate\"]\n",
    "mean = serror_rate.mean()\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target label\n",
    "\n",
    "data.loc[:,'serror_rate'] = (data.loc[:,'serror_rate'] > 0.0016060344473219086).astype(int)\n",
    "print(data.loc[:,'serror_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features:\n",
    "\n",
    "target_label = 'serror_rate'\n",
    "non_features = ['protocol_type', 'service', 'flag']\n",
    "feature_labels = [x for x in data.columns if x not in [target_label] + non_features]\n",
    "\n",
    "# One-hot encode inputs\n",
    "\n",
    "data_expanded = pd.get_dummies(data, drop_first=True)\n",
    "print('DataFrame one-hot-expanded shape: {}'.format(data_expanded.shape))\n",
    "\n",
    "# Get target and original x-matrix\n",
    "\n",
    "y = data[target_label]\n",
    "x = data.as_matrix(columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the model on the training sets only\n",
    "\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Determine your model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "#Accuracy\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Modify data by handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_sample(x, y)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Use the same mode on updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, \n",
    "                                  test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the model on the training sets only\n",
    "\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) What is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "#Accuracy\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) Describe your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "    \n",
    "    The accuracy increased significantly after using the SMOTE method for soliving for class imbalance. SMOTE stands for Synthetic Minority Oversampling Technique. It combines informed oversampling of the minority class with random undersampling of the majority class. In this case, my original model had extreme class imbalance. The SMOTE method increased my model accuarcy to 0.53."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
